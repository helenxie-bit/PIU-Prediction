{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-10T05:36:53.179502Z",
     "iopub.status.busy": "2024-12-10T05:36:53.179122Z",
     "iopub.status.idle": "2024-12-10T05:36:53.184447Z",
     "shell.execute_reply": "2024-12-10T05:36:53.183486Z",
     "shell.execute_reply.started": "2024-12-10T05:36:53.179469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T05:36:53.186189Z",
     "iopub.status.busy": "2024-12-10T05:36:53.185935Z",
     "iopub.status.idle": "2024-12-10T05:36:53.200275Z",
     "shell.execute_reply": "2024-12-10T05:36:53.199545Z",
     "shell.execute_reply.started": "2024-12-10T05:36:53.186142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on GPU: tensor([1., 2., 3.], device='cuda:0')\n",
      "GPU Tensor Sum: 6.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "x = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
    "print(f\"Tensor on GPU: {x}\")\n",
    "print(f\"GPU Tensor Sum: {x.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T05:36:53.201641Z",
     "iopub.status.busy": "2024-12-10T05:36:53.201289Z",
     "iopub.status.idle": "2024-12-10T05:36:53.218269Z",
     "shell.execute_reply": "2024-12-10T05:36:53.217593Z",
     "shell.execute_reply.started": "2024-12-10T05:36:53.201608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# KNN & Mutual Information Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T05:36:53.219786Z",
     "iopub.status.busy": "2024-12-10T05:36:53.219461Z",
     "iopub.status.idle": "2024-12-10T06:02:30.426329Z",
     "shell.execute_reply": "2024-12-10T06:02:30.425460Z",
     "shell.execute_reply.started": "2024-12-10T05:36:53.219752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing alpha = 0.0\n",
      "Alpha: 0.0, Accuracy: 0.5850, Weighted Kappa: 0.4517\n",
      "Testing alpha = 0.1\n",
      "Alpha: 0.1, Accuracy: 0.6033, Weighted Kappa: 0.4252\n",
      "Testing alpha = 0.2\n",
      "Alpha: 0.2, Accuracy: 0.5832, Weighted Kappa: 0.4381\n",
      "Testing alpha = 0.3\n",
      "Alpha: 0.3, Accuracy: 0.5978, Weighted Kappa: 0.4192\n",
      "Testing alpha = 0.4\n",
      "Alpha: 0.4, Accuracy: 0.5905, Weighted Kappa: 0.4260\n",
      "Testing alpha = 0.5\n",
      "Alpha: 0.5, Accuracy: 0.5887, Weighted Kappa: 0.3859\n",
      "Testing alpha = 0.6\n",
      "Alpha: 0.6, Accuracy: 0.5923, Weighted Kappa: 0.3852\n",
      "Testing alpha = 0.7\n",
      "Alpha: 0.7, Accuracy: 0.5868, Weighted Kappa: 0.3681\n",
      "Testing alpha = 0.8\n",
      "Alpha: 0.8, Accuracy: 0.6051, Weighted Kappa: 0.3693\n",
      "Testing alpha = 0.9\n",
      "Alpha: 0.9, Accuracy: 0.5868, Weighted Kappa: 0.3779\n",
      "Testing alpha = 1.0\n",
      "Alpha: 1.0, Accuracy: 0.5759, Weighted Kappa: 0.3589\n",
      "    alpha  accuracy  weighted_kappa\n",
      "0     0.0  0.585009        0.451738\n",
      "1     0.1  0.603291        0.425250\n",
      "2     0.2  0.583181        0.438057\n",
      "3     0.3  0.597806        0.419156\n",
      "4     0.4  0.590494        0.426023\n",
      "5     0.5  0.588665        0.385864\n",
      "6     0.6  0.592322        0.385170\n",
      "7     0.7  0.586837        0.368141\n",
      "8     0.8  0.605119        0.369338\n",
      "9     0.9  0.586837        0.377852\n",
      "10    1.0  0.575868        0.358877\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset/train_re.csv')\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = data.drop(columns=['PCIAT-PCIAT_Total','sii'])\n",
    "y = data['sii']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "\n",
    "# Define the enhanced neural network\n",
    "class EnhancedClassificationNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_rate=0.5, negative_slope=0.01):\n",
    "        super(EnhancedClassificationNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_sizes[2])\n",
    "        self.fc4 = nn.Linear(hidden_sizes[2], num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = nn.LeakyReLU(negative_slope)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define Weighted Kappa Loss\n",
    "class WeightedKappaLoss(nn.Module):\n",
    "    def __init__(self, num_classes, weight_type=\"quadratic\"):\n",
    "        super(WeightedKappaLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.weight_type = weight_type\n",
    "        self.weights = self._create_weights()\n",
    "\n",
    "    def _create_weights(self):\n",
    "        weights = torch.zeros((self.num_classes, self.num_classes))\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                if self.weight_type == \"linear\":\n",
    "                    weights[i, j] = 1 - abs(i - j) / (self.num_classes - 1)\n",
    "                elif self.weight_type == \"quadratic\":\n",
    "                    weights[i, j] = 1 - ((i - j) ** 2) / ((self.num_classes - 1) ** 2)\n",
    "        return weights\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Move weights to the same device as logits\n",
    "        self.weights = self.weights.to(logits.device)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        conf_matrix = torch.zeros((self.num_classes, self.num_classes), device=logits.device)\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                conf_matrix[i, j] = torch.sum((targets == i).float() * probs[:, j])\n",
    "        conf_matrix = conf_matrix / conf_matrix.sum()\n",
    "        P_o = torch.sum(conf_matrix * self.weights)\n",
    "        row_sums = conf_matrix.sum(dim=1)\n",
    "        col_sums = conf_matrix.sum(dim=0)\n",
    "        E = torch.outer(row_sums, col_sums)\n",
    "        P_e = torch.sum(E * self.weights)\n",
    "        kappa_w = (P_o - P_e) / (1 - P_e + 1e-7)\n",
    "        loss = 1 - kappa_w\n",
    "        return loss\n",
    "\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha, num_classes, weight_type=\"quadratic\"):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.kappa_loss = WeightedKappaLoss(num_classes=num_classes, weight_type=weight_type)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce = self.ce_loss(logits, targets)\n",
    "        kappa = self.kappa_loss(logits, targets)\n",
    "        return self.alpha * ce + (1 - self.alpha) * kappa\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [128, 64, 32]\n",
    "num_classes = len(y.unique())\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# List to store results for each alpha\n",
    "results = []\n",
    "\n",
    "# Range of alpha values to test\n",
    "alpha_values = [round(x * 0.1, 1) for x in range(11)]\n",
    "\n",
    "# Loop through alpha values\n",
    "for alpha in alpha_values:\n",
    "    print(f\"Testing alpha = {alpha}\")\n",
    "    \n",
    "    # Reset model, loss, and optimizer\n",
    "    model = EnhancedClassificationNN(input_size, hidden_sizes, num_classes, dropout_rate).to(device)\n",
    "    criterion = HybridLoss(alpha=alpha, num_classes=num_classes, weight_type=\"quadratic\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # DataLoader for batching\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        true_labels = y_test_tensor.cpu().numpy()\n",
    "        predicted_labels = predicted.cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights=\"quadratic\")\n",
    "    print(f\"Alpha: {alpha}, Accuracy: {accuracy:.4f}, Weighted Kappa: {weighted_kappa:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"weighted_kappa\": weighted_kappa\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(results_df)\n",
    "\n",
    "# model = EnhancedClassificationNN(input_size, hidden_sizes, num_classes, dropout_rate).to(device)\n",
    "# alpha = 0.2\n",
    "# criterion = HybridLoss(alpha=alpha, num_classes=num_classes, weight_type=\"quadratic\")\n",
    "\n",
    "# # Training loop\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # DataLoader for batching\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Training the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         # Ensure data is on the correct device\n",
    "#         batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(batch_X)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# # Evaluating the model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test_tensor)\n",
    "#     _, predicted = torch.max(outputs, 1)\n",
    "#     true_labels = y_test_tensor.cpu().numpy()\n",
    "#     predicted_labels = predicted.cpu().numpy()\n",
    "\n",
    "# # Validation metrics\n",
    "# accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "# weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights=\"quadratic\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}, Weighted Kappa (sklearn): {weighted_kappa:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T06:02:30.428262Z",
     "iopub.status.busy": "2024-12-10T06:02:30.427979Z",
     "iopub.status.idle": "2024-12-10T06:02:30.432371Z",
     "shell.execute_reply": "2024-12-10T06:02:30.431505Z",
     "shell.execute_reply.started": "2024-12-10T06:02:30.428236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Regression Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T06:02:30.433970Z",
     "iopub.status.busy": "2024-12-10T06:02:30.433727Z",
     "iopub.status.idle": "2024-12-10T06:28:14.771309Z",
     "shell.execute_reply": "2024-12-10T06:28:14.770437Z",
     "shell.execute_reply.started": "2024-12-10T06:02:30.433946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing alpha = 0.0\n",
      "Alpha: 0.0, Accuracy: 0.5814, Weighted Kappa: 0.4565\n",
      "Testing alpha = 0.1\n",
      "Alpha: 0.1, Accuracy: 0.5850, Weighted Kappa: 0.4165\n",
      "Testing alpha = 0.2\n",
      "Alpha: 0.2, Accuracy: 0.5960, Weighted Kappa: 0.4198\n",
      "Testing alpha = 0.3\n",
      "Alpha: 0.3, Accuracy: 0.6088, Weighted Kappa: 0.4624\n",
      "Testing alpha = 0.4\n",
      "Alpha: 0.4, Accuracy: 0.6106, Weighted Kappa: 0.4179\n",
      "Testing alpha = 0.5\n",
      "Alpha: 0.5, Accuracy: 0.6088, Weighted Kappa: 0.4214\n",
      "Testing alpha = 0.6\n",
      "Alpha: 0.6, Accuracy: 0.6088, Weighted Kappa: 0.4351\n",
      "Testing alpha = 0.7\n",
      "Alpha: 0.7, Accuracy: 0.5996, Weighted Kappa: 0.4268\n",
      "Testing alpha = 0.8\n",
      "Alpha: 0.8, Accuracy: 0.6271, Weighted Kappa: 0.4476\n",
      "Testing alpha = 0.9\n",
      "Alpha: 0.9, Accuracy: 0.5941, Weighted Kappa: 0.3613\n",
      "Testing alpha = 1.0\n",
      "Alpha: 1.0, Accuracy: 0.5996, Weighted Kappa: 0.3954\n",
      "    alpha  accuracy  weighted_kappa\n",
      "0     0.0  0.581353        0.456517\n",
      "1     0.1  0.585009        0.416479\n",
      "2     0.2  0.595978        0.419789\n",
      "3     0.3  0.608775        0.462353\n",
      "4     0.4  0.610603        0.417934\n",
      "5     0.5  0.608775        0.421398\n",
      "6     0.6  0.608775        0.435144\n",
      "7     0.7  0.599634        0.426819\n",
      "8     0.8  0.627057        0.447611\n",
      "9     0.9  0.594150        0.361285\n",
      "10    1.0  0.599634        0.395410\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('dataset/train_rf.csv')\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = data.drop(columns=['PCIAT-PCIAT_Total','sii'])\n",
    "y = data['sii']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "\n",
    "# Define the enhanced neural network\n",
    "class EnhancedClassificationNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_rate=0.5, negative_slope=0.01):\n",
    "        super(EnhancedClassificationNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_sizes[2])\n",
    "        self.fc4 = nn.Linear(hidden_sizes[2], num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.activation = nn.LeakyReLU(negative_slope)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define Weighted Kappa Loss\n",
    "class WeightedKappaLoss(nn.Module):\n",
    "    def __init__(self, num_classes, weight_type=\"quadratic\"):\n",
    "        super(WeightedKappaLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.weight_type = weight_type\n",
    "        self.weights = self._create_weights()\n",
    "\n",
    "    def _create_weights(self):\n",
    "        weights = torch.zeros((self.num_classes, self.num_classes))\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                if self.weight_type == \"linear\":\n",
    "                    weights[i, j] = 1 - abs(i - j) / (self.num_classes - 1)\n",
    "                elif self.weight_type == \"quadratic\":\n",
    "                    weights[i, j] = 1 - ((i - j) ** 2) / ((self.num_classes - 1) ** 2)\n",
    "        return weights\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Move weights to the same device as logits\n",
    "        self.weights = self.weights.to(logits.device)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        conf_matrix = torch.zeros((self.num_classes, self.num_classes), device=logits.device)\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                conf_matrix[i, j] = torch.sum((targets == i).float() * probs[:, j])\n",
    "        conf_matrix = conf_matrix / conf_matrix.sum()\n",
    "        P_o = torch.sum(conf_matrix * self.weights)\n",
    "        row_sums = conf_matrix.sum(dim=1)\n",
    "        col_sums = conf_matrix.sum(dim=0)\n",
    "        E = torch.outer(row_sums, col_sums)\n",
    "        P_e = torch.sum(E * self.weights)\n",
    "        kappa_w = (P_o - P_e) / (1 - P_e + 1e-7)\n",
    "        loss = 1 - kappa_w\n",
    "        return loss\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, alpha, num_classes, weight_type=\"quadratic\"):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.kappa_loss = WeightedKappaLoss(num_classes=num_classes, weight_type=weight_type)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce = self.ce_loss(logits, targets)\n",
    "        kappa = self.kappa_loss(logits, targets)\n",
    "        return self.alpha * ce + (1 - self.alpha) * kappa\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [128, 64, 32]\n",
    "num_classes = len(y.unique())\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "# List to store results for each alpha\n",
    "results = []\n",
    "\n",
    "# Range of alpha values to test\n",
    "alpha_values = [round(x * 0.1, 1) for x in range(11)]\n",
    "\n",
    "# Loop through alpha values\n",
    "for alpha in alpha_values:\n",
    "    print(f\"Testing alpha = {alpha}\")\n",
    "    \n",
    "    # Reset model, loss, and optimizer\n",
    "    model = EnhancedClassificationNN(input_size, hidden_sizes, num_classes, dropout_rate).to(device)\n",
    "    criterion = HybridLoss(alpha=alpha, num_classes=num_classes, weight_type=\"quadratic\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # DataLoader for batching\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        true_labels = y_test_tensor.cpu().numpy()\n",
    "        predicted_labels = predicted.cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights=\"quadratic\")\n",
    "    print(f\"Alpha: {alpha}, Accuracy: {accuracy:.4f}, Weighted Kappa: {weighted_kappa:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"weighted_kappa\": weighted_kappa\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "# model = EnhancedClassificationNN(input_size, hidden_sizes, num_classes, dropout_rate).to(device)\n",
    "# alpha = 0.2\n",
    "# criterion = HybridLoss(alpha=alpha, num_classes=num_classes, weight_type=\"quadratic\")\n",
    "\n",
    "# # Training loop\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # DataLoader for batching\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Training the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         # Ensure data is on the correct device\n",
    "#         batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(batch_X)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# # Evaluating the model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test_tensor)\n",
    "#     _, predicted = torch.max(outputs, 1)\n",
    "#     true_labels = y_test_tensor.cpu().numpy()\n",
    "#     predicted_labels = predicted.cpu().numpy()\n",
    "\n",
    "# # Validation metrics\n",
    "# accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "# weighted_kappa = cohen_kappa_score(true_labels, predicted_labels, weights=\"quadratic\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}, Weighted Kappa (sklearn): {weighted_kappa:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6263157,
     "sourceId": 10146415,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
