{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-04T19:37:48.529895Z",
     "iopub.status.busy": "2024-12-04T19:37:48.529442Z",
     "iopub.status.idle": "2024-12-04T19:37:48.536889Z",
     "shell.execute_reply": "2024-12-04T19:37:48.535223Z",
     "shell.execute_reply.started": "2024-12-04T19:37:48.529860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/envs/kubeflow/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/homebrew/anaconda3/envs/kubeflow/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/envs/kubeflow/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/envs/kubeflow/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/envs/kubeflow/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/kubeflow/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/anaconda3/envs/kubeflow/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.0-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:37:48.540393Z",
     "iopub.status.busy": "2024-12-04T19:37:48.539757Z",
     "iopub.status.idle": "2024-12-04T19:37:48.549950Z",
     "shell.execute_reply": "2024-12-04T19:37:48.548285Z",
     "shell.execute_reply.started": "2024-12-04T19:37:48.540354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "target = [\"PCIAT-PCIAT_Total\", \"sii\"]\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:37:48.552329Z",
     "iopub.status.busy": "2024-12-04T19:37:48.551931Z",
     "iopub.status.idle": "2024-12-04T19:37:48.612298Z",
     "shell.execute_reply": "2024-12-04T19:37:48.611147Z",
     "shell.execute_reply.started": "2024-12-04T19:37:48.552291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the current directory of the notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct the absolute path to the dataset folder\n",
    "dataset_dir = os.path.abspath(os.path.join(current_dir, 'dataset'))\n",
    "\n",
    "# Construct the absolute path to train.csv\n",
    "train_csv_path = os.path.join(dataset_dir, 'train.csv')\n",
    "\n",
    "# Load Tabular Data\n",
    "train = pd.read_csv(train_csv_path, index_col=None)\n",
    "\n",
    "#print(train.info())\n",
    "#print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:37:48.615285Z",
     "iopub.status.busy": "2024-12-04T19:37:48.614940Z",
     "iopub.status.idle": "2024-12-04T19:37:48.630379Z",
     "shell.execute_reply": "2024-12-04T19:37:48.628976Z",
     "shell.execute_reply.started": "2024-12-04T19:37:48.615252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine \"PAQ_A-PAQ_A_Total\" and \"PAQ_C-PAQ_C_Total\" Together\n",
    "train = train.assign(\n",
    "    PAQ_Total=train[\"PAQ_A-PAQ_A_Total\"].combine_first(train[\"PAQ_C-PAQ_C_Total\"])\n",
    ").drop(columns=[\"PAQ_A-PAQ_A_Total\", \"PAQ_C-PAQ_C_Total\"])\n",
    "\n",
    "#print(train.info())\n",
    "#print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:37:48.632636Z",
     "iopub.status.busy": "2024-12-04T19:37:48.632127Z",
     "iopub.status.idle": "2024-12-04T19:37:48.649913Z",
     "shell.execute_reply": "2024-12-04T19:37:48.648543Z",
     "shell.execute_reply.started": "2024-12-04T19:37:48.632583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "# Step 1: Removed features containing “season”\n",
    "exclude_season = [col for col in train.columns if \"season\" in col.lower()]\n",
    "\n",
    "# Step 2: Dropped features with more than 70% missing values\n",
    "missing_rate = train.isnull().mean() * 100\n",
    "exclude_missing = list(missing_rate[missing_rate > 70].index)\n",
    "\n",
    "# Step 3: Excluded categorical features derived from other features (e.g., “FGC-FGC_CU_Zone”)\n",
    "exclude_zone = [col for col in train.columns if \"zone\" in col.lower()]\n",
    "\n",
    "# Step 4: Remove PCIAT features from train (keep \"PCIAT-PCIAT_Total\" for ridge regression)\n",
    "exclude_PCIAT = [\"PCIAT-Season\"]+ [col for col in train.columns if \"PCIAT-PCIAT_\" in col and col[-2:].isdigit()]\n",
    "\n",
    "train = train.drop(columns=exclude_season + exclude_missing + exclude_zone + exclude_PCIAT)\n",
    "\n",
    "#print(train.info())\n",
    "#print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:37:48.651822Z",
     "iopub.status.busy": "2024-12-04T19:37:48.651397Z",
     "iopub.status.idle": "2024-12-04T19:37:48.665612Z",
     "shell.execute_reply": "2024-12-04T19:37:48.664401Z",
     "shell.execute_reply.started": "2024-12-04T19:37:48.651784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Handling Outliers\n",
    "outliers_id = ['cedf96c5', 'e252dcb6', '83525bbe']\n",
    "train = train[~train['id'].isin(outliers_id)]\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "\n",
    "#print(train.info())\n",
    "#print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T19:37:48.667732Z",
     "iopub.status.busy": "2024-12-04T19:37:48.667330Z",
     "iopub.status.idle": "2024-12-04T19:37:53.108913Z",
     "shell.execute_reply": "2024-12-04T19:37:53.107332Z",
     "shell.execute_reply.started": "2024-12-04T19:37:48.667696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Handling Missing Data\n",
    "# Step 1: Dropped rows with missing labels\n",
    "train = train.dropna(subset=[\"sii\"])\n",
    "\n",
    "# Step 2: Imputation:\n",
    "# 1) Method1: Imputed missing values in other columns using regression based on age\n",
    "def fill_missing_values_with_regression(train, features, predictor):\n",
    "    train_imputed = train.copy()\n",
    "    \n",
    "    for feature in features:\n",
    "        # Skip columns with no missing values\n",
    "        if train_imputed[feature].isna().sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Extract non-missing data for training the regression model\n",
    "        non_missing_data = train_imputed[train_imputed[feature].notna()]\n",
    "        if len(non_missing_data) < 2: # Skip if insufficient data\n",
    "            print(f\"Skipping feature '{feature}' due to insufficient data.\")\n",
    "            continue\n",
    "\n",
    "        X_train = non_missing_data[[predictor]]\n",
    "        y_train = non_missing_data[feature]\n",
    "        \n",
    "        # Train a linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Impute missing values in training data\n",
    "        train_missing = train_imputed[train_imputed[feature].isna()]\n",
    "        if not train_missing.empty:\n",
    "            X_train_missing = train_missing[[predictor]]\n",
    "            train_imputed.loc[train_imputed[feature].isna(), feature] = model.predict(X_train_missing)\n",
    "\n",
    "    return train_imputed\n",
    "\n",
    "# 2) Method2: Imputed missing values in continuous variables - KNN; missing values in categorical variables - random forest\n",
    "def fill_missing_values_with_knn(train, continuous_features):\n",
    "    train_imputed = train.copy()\n",
    "    \n",
    "    # Scale continuous features\n",
    "    scaler = StandardScaler()\n",
    "    train_continuous_scaled = scaler.fit_transform(train[continuous_features])\n",
    "\n",
    "    # Apply KNN imputation\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    train_continuous_imputed = imputer.fit_transform(train_continuous_scaled)\n",
    "    \n",
    "    # Replace the continuous features in the dataset with the imputed values\n",
    "    train_imputed[continuous_features] = scaler.inverse_transform(train_continuous_imputed)\n",
    "\n",
    "    return train_imputed\n",
    "\n",
    "def fill_missing_values_with_random_forest(train, category_features, continuous_features):\n",
    "    train_imputed = train.copy()\n",
    "\n",
    "    for feature in category_features:\n",
    "        # Skip features with no missing values\n",
    "        if train_imputed[feature].isna().sum() == 0:\n",
    "                continue\n",
    "\n",
    "        # Drop rows where feature contains missing value\n",
    "        df = train_imputed.dropna(subset=[feature])\n",
    "\n",
    "        # Calculate mutual information with continous features\n",
    "        mi_continuous = mutual_info_classif(df[continuous_features], df[feature], discrete_features=False)\n",
    "        mi_continuous_df = pd.DataFrame({\n",
    "            'Feature': continuous_features,\n",
    "            'Mutual Information': mi_continuous\n",
    "        })\n",
    "\n",
    "        # Calculate mutual information with other category features\n",
    "        other_category_features = [col for col in category_features if col != feature]\n",
    "        df_category = df.dropna(subset=other_category_features) # Drop rows where other category features contain missing values\n",
    "        mi_category = mutual_info_classif(df_category[other_category_features], df_category[feature], discrete_features=True)\n",
    "        mi_category_df = pd.DataFrame({\n",
    "            'Feature': other_category_features,\n",
    "            'Mutual Information': mi_category\n",
    "        })\n",
    "\n",
    "        # Combine mutual information results\n",
    "        mi_combined_df = pd.concat([mi_continuous_df, mi_category_df]).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "        #print(f\"{feature}:\")\n",
    "        #print(mi_combined_df)\n",
    "\n",
    "        # Filter predictors with mutual information > 0.08\n",
    "        filtered_predictors = mi_combined_df[mi_combined_df['Mutual Information'] > 0.08]['Feature'].tolist()\n",
    "\n",
    "        # Handle multicollinearity\n",
    "        corr_matrix = df[filtered_predictors].corr()\n",
    "        high_corr_pairs = [\n",
    "            (col1, col2)\n",
    "            for col1 in corr_matrix.columns\n",
    "            for col2 in corr_matrix.columns\n",
    "            if col1 != col2 and abs(corr_matrix[col1][col2]) > 0.8\n",
    "        ]\n",
    "\n",
    "        # Resolve multicollinearity by keeping features with higher mutual information\n",
    "        mi_dict = pd.concat([mi_continuous_df, mi_category_df]).set_index('Feature')['Mutual Information'].to_dict()\n",
    "        features_to_drop = set()\n",
    "        for col1, col2 in high_corr_pairs:\n",
    "            if mi_dict.get(col1, 0) >= mi_dict.get(col2, 0):\n",
    "                features_to_drop.add(col2)\n",
    "            else:\n",
    "                features_to_drop.add(col1)\n",
    "        final_predictors = [col for col in filtered_predictors if col not in features_to_drop]\n",
    "\n",
    "        #print(f\"feature: {feature}; final predictors: {final_predictors}\")\n",
    "\n",
    "        # Train a Random Forest classifier\n",
    "        X = df[final_predictors]\n",
    "        y = df[feature]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "        model = RandomForestClassifier(random_state=SEED)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Predict missing values in train\n",
    "        missing_idx_train = train_imputed[feature].isnull()\n",
    "        train_imputed.loc[missing_idx_train, feature] = model.predict(train_imputed.loc[missing_idx_train, final_predictors])\n",
    "    \n",
    "        \n",
    "    return train_imputed\n",
    "\n",
    "# Filter category & continuous features\n",
    "data_dict = pd.read_csv(\"dataset/data_dictionary.csv\")\n",
    "category_fields = data_dict[data_dict[\"Type\"].str.contains(\"categorical\", case=False, na=False)][\"Field\"]\n",
    "category_features = [col for col in train.columns if col in category_fields.values and col not in target]\n",
    "continuous_features = [col for col in train.columns if col not in category_fields.values and col not in target]\n",
    "\n",
    "# Apply imputation method1\n",
    "train_re = fill_missing_values_with_regression(train, [col for col in train.columns if col not in target], \"Basic_Demos-Age\")\n",
    "\n",
    "# Apply imputation method2\n",
    "train_knn = fill_missing_values_with_knn(train, continuous_features)\n",
    "train_rf = fill_missing_values_with_random_forest(train_knn, category_features, continuous_features)\n",
    "\n",
    "# Save results\n",
    "train_re.to_csv('dataset/train_re.csv', index=False)\n",
    "train_rf.to_csv('dataset/train_rf.csv', index=False)\n",
    "\n",
    "#print(train_re.info())\n",
    "#print(train_rf.info())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "fp221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
